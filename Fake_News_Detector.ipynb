{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqcV7Oa8kP62",
        "outputId": "554be18f-4bb0-4a13-d9e3-413f539101d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>source</th>\n",
              "      <th>author</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Foreign Democrat final.</td>\n",
              "      <td>more tax development both store agreement lawy...</td>\n",
              "      <td>2023-03-10</td>\n",
              "      <td>NY Times</td>\n",
              "      <td>Paula George</td>\n",
              "      <td>Politics</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To offer down resource great point.</td>\n",
              "      <td>probably guess western behind likely next inve...</td>\n",
              "      <td>2022-05-25</td>\n",
              "      <td>Fox News</td>\n",
              "      <td>Joseph Hill</td>\n",
              "      <td>Politics</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Himself church myself carry.</td>\n",
              "      <td>them identify forward present success risk sev...</td>\n",
              "      <td>2022-09-01</td>\n",
              "      <td>CNN</td>\n",
              "      <td>Julia Robinson</td>\n",
              "      <td>Business</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You unit its should.</td>\n",
              "      <td>phone which item yard Republican safe where po...</td>\n",
              "      <td>2023-02-07</td>\n",
              "      <td>Reuters</td>\n",
              "      <td>Mr. David Foster DDS</td>\n",
              "      <td>Science</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Billion believe employee summer how.</td>\n",
              "      <td>wonder myself fact difficult course forget exa...</td>\n",
              "      <td>2023-04-03</td>\n",
              "      <td>CNN</td>\n",
              "      <td>Austin Walker</td>\n",
              "      <td>Technology</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  title  \\\n",
              "0               Foreign Democrat final.   \n",
              "1   To offer down resource great point.   \n",
              "2          Himself church myself carry.   \n",
              "3                  You unit its should.   \n",
              "4  Billion believe employee summer how.   \n",
              "\n",
              "                                                text        date    source  \\\n",
              "0  more tax development both store agreement lawy...  2023-03-10  NY Times   \n",
              "1  probably guess western behind likely next inve...  2022-05-25  Fox News   \n",
              "2  them identify forward present success risk sev...  2022-09-01       CNN   \n",
              "3  phone which item yard Republican safe where po...  2023-02-07   Reuters   \n",
              "4  wonder myself fact difficult course forget exa...  2023-04-03       CNN   \n",
              "\n",
              "                 author    category label  \n",
              "0          Paula George    Politics  real  \n",
              "1           Joseph Hill    Politics  fake  \n",
              "2        Julia Robinson    Business  fake  \n",
              "3  Mr. David Foster DDS     Science  fake  \n",
              "4         Austin Walker  Technology  fake  "
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd \n",
        "df = pd.read_csv(\"fake_news_dataset.csv\") \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "i7ZfYf7Vj3j0"
      },
      "outputs": [],
      "source": [
        "# Cell 2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "E0fyOBJckcGo",
        "outputId": "14ed6feb-72c4-45c3-8b34-80fcc068db1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17052"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 3\n",
        "all_authors = list(df['author'].unique())\n",
        "len(all_authors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 4\n",
        "all_categories = list(df['category'].unique())\n",
        "len(all_categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Q2AZbSkik1",
        "outputId": "392fbe4d-89ae-4e62-95d9-1b37b3c4e920"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "category\n",
              "Health           2922\n",
              "Entertainment    2889\n",
              "Technology       2882\n",
              "Sports           2867\n",
              "Business         2849\n",
              "Politics         2802\n",
              "Science          2789\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 5\n",
        "df['category'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0VZZqvzkt3G",
        "outputId": "c79bb522-352f-47ce-d070-3b78b0f5e839"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['title', 'text', 'date', 'source', 'author', 'category', 'label'], dtype='object')"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 6\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7\n",
        "df = df[['text', 'label']]  # we are going to make prediction only upon text\n",
        "df.dropna(inplace=True)     # remove rows with null values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8\n",
        "df['label'] = df['label'].map({'fake': 0, 'real': 1})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caY7O_X8k5fF",
        "outputId": "cc19be24-a272-46a8-b9c5-c274ea081eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 9\n",
        "df['label'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "MZF6OQislJ0u",
        "outputId": "e38bee6e-264c-41b9-8e2c-9e20a3228e4c"
      },
      "outputs": [],
      "source": [
        "# Cell 10\n",
        "stop_words = set([\n",
        "    \"the\",\"and\",\"is\",\"in\",\"to\",\"a\",\"of\",\"for\",\"on\",\"with\",\"as\",\"by\",\n",
        "    \"at\",\"an\",\"this\",\"that\",\"it\",\"from\",\"be\",\"has\",\"was\",\"are\",\"or\",\n",
        "    \"but\",\"its\",\"have\",\"they\",\"their\",\"will\",\"can\",\"which\",\"about\"\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    cleaned_words = []\n",
        "    for word in text.split():\n",
        "        # keep only alphabetic words longer than 2 chars and not in stop words\n",
        "        word = ''.join([c for c in word if 'a' <= c <= 'z'])\n",
        "        if len(word) > 2 and word not in stop_words:\n",
        "            cleaned_words.append(word)\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OescuZkzlNbr",
        "outputId": "f33c7791-df7e-465d-e210-cfff6571a9ce"
      },
      "outputs": [],
      "source": [
        "# Cell 12\n",
        "def build_vocab(texts):\n",
        "    vocab = {}\n",
        "    index = 0\n",
        "    for text in texts:\n",
        "        for word in text.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = index\n",
        "                index += 1\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "b0bYv0cDm_Sz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size after cleaning: 928\n"
          ]
        }
      ],
      "source": [
        "# Cell 13\n",
        "vocab = build_vocab(X_train)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size after cleaning: {vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'road listen add question main head worker general source report cold process training provide bag election she visit lawyer security store product relate consumer inside above economic box notice nice phone until color edge month situation development realize relate several determine rule man visit main level politics people mind out would who ever able miss rule one down set around throw war full job several issue just force film manager very pass animal none stay age reason bank town team already near left husband suffer natural stock instead environment indicate provide including soldier cup others effect focus realize son identify inside finally middle decide degree whom mention hear stay music consumer beautiful kind each court painting billion health money thus what conference pay provide purpose long five same worker site another week mean test give benefit land need season factor positive partner whatever discover mission determine area health oil partner control benefit task our lead allow admit school consumer executive shoulder key wonder story however spend recognize while next clearly industry lay who trouble same level them movie popular clearly animal affect college approach audience your arm story business role agent else yard discussion hope avoid myself positive yeah doctor necessary rest result thought cultural role just authority few base approach hospital few out lawyer per newspaper affect back whether language plan interview crime much race republican consumer fund reveal move week main consider unit health again agent force two along now office camera only skill note'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 14\n",
        "X_train[3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 15\n",
        "len(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16000.0"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 16\n",
        "df.shape[0]*0.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17\n",
        "def compute_tf(texts, vocab):\n",
        "    tf = np.zeros((len(texts), len(vocab)))  # tf is matrix of size N x vocab_size\n",
        "    for i, text in enumerate(texts):\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                tf[i, vocab[word]] += 1\n",
        "        # if len(words) > 0:\n",
        "        #     tf[i] = tf[i] / len(words)\n",
        "    return tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 18\n",
        "tf_train = compute_tf(X_train, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 3., 2., 1.,\n",
              "       2., 4., 1., 1., 1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 1., 1., 1., 2., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 2., 1., 1., 1., 2., 1., 1., 3., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n",
              "       2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 19\n",
        "tf_train[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 20\n",
        "def compute_idf(texts, vocab):\n",
        "    idf = np.zeros(len(vocab))\n",
        "    N = len(texts)\n",
        "    for word, idx in vocab.items():\n",
        "        doc_count = 0\n",
        "        for text in texts:\n",
        "            if word in text.split():\n",
        "                doc_count += 1\n",
        "        if doc_count == 0:\n",
        "            idf[idx] = 0\n",
        "        else:\n",
        "            idf[idx] = N / doc_count\n",
        "    return idf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 20\n",
        "def compute_idf(texts, vocab):\n",
        "    idf = np.zeros(len(vocab))\n",
        "    N = len(texts)\n",
        "    for word, idx in vocab.items():\n",
        "        doc_count = 0\n",
        "        for text in texts:\n",
        "            if word in text.split():\n",
        "                doc_count += 1\n",
        "        if doc_count == 0:\n",
        "            idf[idx] = 0\n",
        "        else:\n",
        "            idf[idx] = N / doc_count\n",
        "    return idf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF shape: (16000, 759)\n"
          ]
        }
      ],
      "source": [
        "# Cell 21\n",
        "# changes from vivid1680\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(\"TF-IDF shape:\", X_train_tfidf.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 22\n",
        "idf_values = tfidf.idf_  # idf values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(2.4936637121553558)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 23\n",
        "idf_values[7]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 24\n",
        "tfidf_train = X_train_tfidf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16000, 759)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 25\n",
        "tfidf_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 26\n",
        "X_test_tfidf = tfidf.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.51175\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.51      0.51      2029\n",
            "           1       0.50      0.52      0.51      1971\n",
            "\n",
            "    accuracy                           0.51      4000\n",
            "   macro avg       0.51      0.51      0.51      4000\n",
            "weighted avg       0.51      0.51      0.51      4000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 27\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train SVM on TF-IDF features\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train_tfidf, y_train)  # use X_train_tfidf\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPRru4E/6pYAtWcsdbctiDl",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
